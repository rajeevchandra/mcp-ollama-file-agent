# ğŸ§  Ollama MCP Local Agent

> Local AI Agents with Tool Use (Filesystem) Powered by Ollama + LangChain + MCP

## ğŸš€ Features

- Run local LLMs like `qwen2:7b`, `mistral`, `phi3`, etc.
- Enable tool use via Model Context Protocol (MCP)
- Fetch and summarize files from your filesystem
- Streamlit UI + Python Agent
- Fully Dockerized & offline-capable

## ğŸ“¦ Project Structure

```
.
â”œâ”€â”€ agent_runner.py
â”œâ”€â”€ file_tool.py
â”œâ”€â”€ mcp-tool.json
â”œâ”€â”€ streamlit_app.py
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ Dockerfile.agent
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ example.txt
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

## ğŸ³ Run with Docker

```bash
docker-compose up --build
```

- Visit Streamlit: http://localhost:8501
- Agent output shown in terminal
- MCP API: http://localhost:3333/docs

## ğŸ“œ License

[MIT](LICENSE)